{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Detener sesión anterior\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Preparación dataset para entrenamiento del modelo predictivo\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898e7b1e-c0ac-4ef4-9c51-fc86ed7b058a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ruta_df_principal = f\"data/resultados/dataset_final.parquet\"\n",
    "ruta_df_reglas= f\"data/resultados/dataset_reglas.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a235cb-b959-4539-93cd-f7f440f1b4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dataset = spark.read.parquet(ruta_df_principal)\n",
    "df_reglas = spark.read.parquet(ruta_df_reglas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75be1e14-26df-460f-b479-79b1d3442f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_reglas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3f66cc-3a19-45f0-be2b-21abf5e64aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, regexp_extract, split\n",
    "\n",
    "# Extraer los elementos individuales de la columna \"items\"\n",
    "df_items = df_reglas.withColumn(\"item\", explode(split(col(\"items\"), \",\")))\n",
    "\n",
    "# Filtrar solo los diagnósticos (Patrón: Letra + 2 números)\n",
    "df_diagnosticos = df_items.withColumn(\"diagnostico\", regexp_extract(col(\"item\"), r\"^[A-Z][0-9]{2}\", 0)) \\\n",
    "                          .filter(col(\"diagnostico\") != \"\")\n",
    "\n",
    "# Mostrar los diagnósticos extraídos\n",
    "df_diagnosticos.select(\"diagnostico\").distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc644029-f45c-4b1e-8f30-8ad39e047605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar el número total de diagnósticos únicos\n",
    "num_diagnosticos = df_diagnosticos.select(\"diagnostico\").distinct().count()\n",
    "\n",
    "print(f\"Total de diagnósticos únicos en las reglas: {num_diagnosticos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f537461-151a-4def-a167-399da9b92cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Se ha llevado a cabo un proceso de extracción y filtrado de diagnósticos a partir del conjunto de reglas de asociación generadas previamente. Dado que el conjunto de datos original contiene una gran diversidad de diagnósticos, muchos de ellos con una granularidad excesiva o con baja representación, se han seleccionado únicamente aquellos que han sido identificados dentro de las reglas de asociación. Para ello, se han analizado los ítems presentes en los antecedentes y consecuentes de las reglas, extrayendo aquellos que cumplen con el patrón de los códigos de diagnóstico (una letra seguida de dos números). Como resultado, se obtiene un total de 1.346 diagnósticos únicos, los cuales serán la base para la construcción del árbol de decisión.\n",
    "\n",
    "Esta selección no solo optimiza el rendimiento computacional al reducir significativamente el volumen de datos procesados, sino que también fortalece la robustez del modelo. Al enfocarnos en diagnósticos que ya han demostrado relaciones significativas dentro de las reglas de asociación, aseguramos que el árbol de decisión se construya sobre patrones clínicamente relevantes. No obstante, se es conciente de que esta estrategia podría excluir ciertos diagnósticos menos frecuentes pero potencialmente importantes, por lo que en futuros estudios sería recomendable evaluar el impacto de esta decisión y considerar estrategias que permitan incluir un mayor espectro de diagnósticos sin comprometer la escalabilidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c6c4ce-035d-4ab3-a029-3ff8efa14cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar total de registros en df_dataset\n",
    "total_registros = df_dataset.count()\n",
    "\n",
    "# Filtrar registros cuyo dominio_icd NO esté en los diagnósticos seleccionados\n",
    "registros_excluidos = df_dataset.filter(~col(\"dominio_icd\").isin([row[\"diagnostico\"] for row in df_diagnosticos.select(\"diagnostico\").distinct().collect()])).count()\n",
    "\n",
    "print(f\"Total de registros en el dataset original: {total_registros}\")\n",
    "print(f\"Registros que serían eliminados: {registros_excluidos}\")\n",
    "print(f\"Porcentaje de registros eliminados: {(registros_excluidos / total_registros) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90e3933-e380-4f43-94d0-3f71934d62dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar solo los registros que contienen diagnósticos dentro del conjunto seleccionado\n",
    "df_dataset_filtrado = df_dataset.filter(col(\"dominio_icd\").isin([row[\"diagnostico\"] for row in df_diagnosticos.select(\"diagnostico\").distinct().collect()]))\n",
    "\n",
    "# Verificar cuántos registros hay después del filtrado\n",
    "registros_finales = df_dataset_filtrado.count()\n",
    "\n",
    "print(f\"Total de registros después del filtrado: {registros_finales}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0cada1-96ed-4f59-bba9-ce529521447a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "valores_nulos = df_dataset_filtrado.select([\n",
    "    (sum(col(c).isNull().cast(\"int\")) / df_dataset_filtrado.count()).alias(c)\n",
    "    for c in df_dataset_filtrado.columns\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9688d9f4-0a65-4b42-b8b7-5ce4f6c74138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Reemplazar valores nulos en 'estado_civil' y 'tipo_seguro' por \"Desconocido\"\n",
    "df_dataset_filtrado = df_dataset_filtrado.withColumn(\n",
    "    \"estado_civil\", when(col(\"estado_civil\").isNull(), \"Desconocido\").otherwise(col(\"estado_civil\"))\n",
    ").withColumn(\n",
    "    \"tipo_seguro\", when(col(\"tipo_seguro\").isNull(), \"Desconocido\").otherwise(col(\"tipo_seguro\"))\n",
    ")\n",
    "\n",
    "# Verificar que no haya valores nulos\n",
    "df_dataset_filtrado.select(\"estado_civil\", \"tipo_seguro\").summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8816f82b-939e-4006-87b3-6d2cb6f5cb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar el número total de pruebas únicas en el dataset\n",
    "total_pruebas_unicas = df_dataset_filtrado.select(\"id_prueba\").distinct().count()\n",
    "print(f\"Total de pruebas únicas en el dataset: {total_pruebas_unicas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8045bb-8546-4080-986d-f937f95ab49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Extraer items de las reglas\n",
    "df_items = df_reglas.withColumn(\"item\", explode(split(col(\"items\"), \",\")))\n",
    "\n",
    "# Filtrar solo los ID de prueba (Patrón exactamente de 5 dígitos)\n",
    "df_pruebas_reglas = df_items.withColumn(\"prueba\", regexp_extract(col(\"item\"), r\"^\\d{5}$\", 0)) \\\n",
    "                            .filter(col(\"prueba\") != \"\") \\\n",
    "                            .select(\"prueba\").distinct()\n",
    "\n",
    "# Contar el número de pruebas en las reglas de asociación\n",
    "pruebas_en_reglas_count = df_pruebas_reglas.count()\n",
    "print(f\"Total de pruebas representadas en las reglas de asociación: {pruebas_en_reglas_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1179e6b9-3638-4261-aaa0-0a6390d4af96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256b544a-2e3c-4dbd-afd8-2bdddc2ac20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creación del dataset para el arbol de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f54310-0a64-48af-a28a-54b34fe7d3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, when, col, array_contains\n",
    "\n",
    "# Obtener el listado único de pruebas\n",
    "pruebas_unicas = df_dataset_filtrado.select(\"id_prueba\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Agrupar por `id_ingreso` y almacenar las pruebas realizadas en una lista\n",
    "df_pruebas = df_dataset_filtrado.groupBy(\"id_ingreso\").agg(\n",
    "    collect_set(\"id_prueba\").alias(\"pruebas_realizadas\")\n",
    ")\n",
    "\n",
    "# Convertir la lista de pruebas en variables dicotómicas (One-Hot Encoding) usando `array_contains()`\n",
    "for p in pruebas_unicas:\n",
    "    df_pruebas = df_pruebas.withColumn(f\"prueba_{p}\", when(array_contains(col(\"pruebas_realizadas\"), p), 1).otherwise(0))\n",
    "\n",
    "# Eliminar la columna de lista de pruebas\n",
    "df_pruebas = df_pruebas.drop(\"pruebas_realizadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pruebas.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c70d20-91b1-4f75-b01e-54f59ac1e24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupar por `id_ingreso` y obtener una lista de dominios de diagnóstico únicos\n",
    "df_diagnosticos = df_dataset_filtrado.groupBy(\"id_ingreso\").agg(\n",
    "    collect_set(\"dominio_icd\").alias(\"dominios\")\n",
    ")\n",
    "\n",
    "df_diagnosticos.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c6ea13-b8d2-4bd5-9f2b-7fb605d1d180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar id_ingreso y edad asegurando unicidad por ingreso\n",
    "df_categorias = df_dataset_filtrado.select(\"id_ingreso\", \"edad\").distinct()\n",
    "\n",
    "# Crear la columna 'edad_categoria' con rangos\n",
    "df_categorias = df_categorias.withColumn(\n",
    "    \"edad_categoria\",\n",
    "    when((df_categorias[\"edad\"] >= 18) & (df_categorias[\"edad\"] <= 39), \"Joven\")\n",
    "    .when((df_categorias[\"edad\"] >= 40) & (df_categorias[\"edad\"] <= 64), \"Adulto joven\")\n",
    "    .when((df_categorias[\"edad\"] >= 65) & (df_categorias[\"edad\"] <= 79), \"Adulto mayor\")\n",
    "    .otherwise(\"Anciano\")  # Para 80+ años\n",
    ")\n",
    "\n",
    "df_categorias.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164c84a1-0572-476f-a03e-3d80e3ce0a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar las variables adicionales asegurando unicidad por `id_ingreso`\n",
    "df_extra = df_dataset_filtrado.select(\n",
    "    \"id_ingreso\", \"sexo\", \"estado_civil\", \"tipo_seguro\", \"grupo_poblacional\", \"muerte_durante_ingreso\"\n",
    ").distinct()\n",
    "\n",
    "df_extra.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero de filas en df_diagnosticos: {df_diagnosticos.count()}\")\n",
    "print(f\"Numero de filas en df_categorias: {df_categorias.count()}\")\n",
    "print(f\"Numero de filas en df_extra: {df_extra.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero de filas en df_dataset: {df_dataset.count()}\")\n",
    "print(f\"Numero de ingresos en df_dataset: {df_dataset.select('id_ingreso').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3669bbae-e758-4a5f-a50e-91e04e2755cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Unir las tablas en un único dataset\n",
    "df_arbol = df_pruebas \\\n",
    "    .join(df_diagnosticos, on=\"id_ingreso\", how=\"inner\") \\\n",
    "    .join(df_categorias, on=\"id_ingreso\", how=\"inner\") \\\n",
    "    .join(df_extra, on=\"id_ingreso\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_arbol.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_arbol.take(1)\n",
    "df_arbol.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbol.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ecffac-7f71-4203-a353-a497a3107dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path_parquet = f\"data/resultados/dataset_arbol.parquet\"\n",
    "\n",
    "# Guardar en formato Parquet\n",
    "df_arbol.write.mode(\"overwrite\").parquet(output_path_parquet)\n",
    "\n",
    "print(f\"Dataset guardado en formato Parquet en: {output_path_parquet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737b78fc-b775-46fe-b347-365eff7a765b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tras filtrar el dataset para conservar únicamente los diagnósticos presentes en las reglas de asociación, se realizan una serie de transformaciones para estructurar la información de manera óptima para el modelo de clasificación basado en árboles de decisión. \n",
    "- Primero, agregamos las pruebas de laboratorio (id_prueba) por paciente (id_ingreso), convirtiéndolas en una lista de pruebas realizadas. \n",
    "- Posteriormente, aplicamos One-Hot Encoding para transformar estas pruebas en variables dicotómicas, de modo que cada prueba se representara con una columna binaria (1 si la prueba fue realizada, 0 en caso contrario), lo que facilita la interpretación y uso en modelos supervisados.\n",
    "- Además, agrupamos los diagnósticos por paciente en una lista (dominios), lo que nos permite capturar la presencia de múltiples condiciones médicas sin perder granularidad.\n",
    "- Se incorporó la variable edad y se categorizó en cuatro grupos clínicamente relevantes: Joven (18-39 años), Adulto Joven (40-64 años), Adulto Mayor (65-79 años) y Anciano (80+ años). \n",
    "- Finalmente, integró la información sociodemográfica relevante (sexo, estado_civil, tipo_seguro, grupo_poblacional, muerte_durante_ingreso) asegurando unicidad por paciente. Todas estas tablas intermedias fueron combinadas en un único dataset (df_arbol) para su uso en el entrenamiento del modelo de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Creacion del dataset_ARBOL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
