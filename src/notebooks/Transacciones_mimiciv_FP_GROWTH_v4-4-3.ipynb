{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbe7d42-4dd9-4466-b735-56e21b080beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Transacciones MIMIC IV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e1cdf5-e44e-4d44-9fc2-4350b7c414b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cargar el fichero .parquet de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da01459e-0774-40e8-928a-da9d2d08f743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dada la magnitud de los datos se hace uso de un dataframe de Spark para convertirlos en un formato adecuado para el análisis de reglas de asociación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, regexp_extract, count\n",
    "\n",
    "# Detener sesión anterior\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Preparación dataset para entrenamiento del modelo predictivo\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f\"./data\"\n",
    "input_path = f\"{base_path}/resultados/dataset_final.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = spark.read.parquet(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce014248-9f6f-4745-a2b9-8d2a552d97fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extraer datos para el algoritmo de Reglas de Asociación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612dde5d-caf3-4e73-912a-79d67d04f347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Consultar con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.createOrReplaceTempView(\"resultados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_association_rules = df_dataset.select('*')\n",
    "df_association_rules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admisiones unicas y que corresponderán a transacciones\n",
    "# Contar las admisiones únicas\n",
    "unique_admissions = spark.sql(\"SELECT COUNT(DISTINCT id_ingreso) AS total_admisiones FROM resultados\")\n",
    "\n",
    "unique_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ages = spark.sql(\"select id_ingreso, edad from resultados group by id_ingreso, edad order by edad\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Asegurar que la columna de edad es numérica\n",
    "df_ages[\"edad\"] = df_ages[\"edad\"].astype(int)\n",
    "\n",
    "# Definir el rango de edades de 20 a 90 con intervalos de 10\n",
    "edad_min = 20\n",
    "edad_max = 90\n",
    "bins = np.arange(edad_min, edad_max + 10, 10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generar histograma\n",
    "df_ages[\"edad\"].hist(\n",
    "    bins=bins, color='#99CC99', alpha=0.7, edgecolor='white', density=True\n",
    ")\n",
    "\n",
    "# Ajustar etiquetas del eje X\n",
    "plt.xticks(\n",
    "    bins[:-1] + 5,\n",
    "    labels=[f\"{int(b)}-{int(b+9)}\" for b in bins[:-1]],\n",
    "    rotation=0\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Edad (Rangos)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribución de Edades\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.xlim(edad_min, edad_max) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_association_rules = df_association_rules.withColumn(\n",
    "    \"edad_categoria\",\n",
    "    when((df_association_rules[\"edad\"] >= 18) & (df_association_rules[\"edad\"] <= 39), \"Joven\")\n",
    "    .when((df_association_rules[\"edad\"] >= 40) & (df_association_rules[\"edad\"] <= 64), \"Adulto joven\")\n",
    "    .when((df_association_rules[\"edad\"] >= 65) & (df_association_rules[\"edad\"] <= 79), \"Adulto mayor\")\n",
    "    .otherwise(\"Anciano\")  # Para 80+ años\n",
    ")\n",
    "\n",
    "df_association_rules.select(\"edad\", \"edad_categoria\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_association_rules = df_association_rules.drop(\"edad\")\n",
    "display(df_association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_association_rules.createOrReplaceTempView(\"resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1be4125-138d-4a73-9ded-d14cff2f46c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tratar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Contar el número de valores nulos por columna\n",
    "null_counts = df_association_rules.select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_association_rules.columns]\n",
    ")\n",
    "\n",
    "# Mostrar el resultado\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_association_rules = df_association_rules.fillna({\n",
    "    \"estado_civil\": \"estado_civil_desconocido\",\n",
    "    \"tipo_seguro\": \"tipo_seguro_desconocido\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_association_rules.createOrReplaceTempView(\"resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb7d3dc-7430-4db4-bc69-a6220e0efaa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformar a la estructura de datos adecuada para el algoritmo de minería de reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, concat_ws\n",
    "\n",
    "# Aplicar la lógica al dataframe en Spark\n",
    "df_items = df_association_rules.groupBy(\"id_ingreso\").agg(\n",
    "    concat_ws(\",\", \n",
    "        collect_set(\"id_prueba\"), \n",
    "        collect_set(\"dominio_icd\"),\n",
    "        collect_set(\"estado_civil\"),\n",
    "        collect_set(\"tipo_seguro\"),\n",
    "        collect_set(\"grupo_poblacional\"),\n",
    "        collect_set(\"muerte_durante_ingreso\"),\n",
    "        collect_set(\"sexo\"),\n",
    "        collect_set(\"edad_categoria\")\n",
    "    ).alias(\"items\")\n",
    ")\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_items.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es un **dataset en formato transaccional**, donde cada transacción (en este caso, un ingreso hospitalario identificado como id_ingreso) esté asociada a un conjunto de ítems (pruebas bioquímicas, diagnósticos, características sociodemográficas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.write.mode(\"overwrite\").parquet(f\"{base_path}/resultados/dataset_reglas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"{base_path}/resultados/dataset_reglas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV\n",
    "df_transactions = spark.read.csv(\n",
    "    csv_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Comprobar si hay valores nulos en las columnas\n",
    "df_transactions.select(\n",
    "    [\n",
    "        F.sum(F.col(\"id_ingreso\").isNull().cast(\"int\")).alias(\"null_id_ingreso\"),\n",
    "        F.sum(F.col(\"items\").isNull().cast(\"int\")).alias(\"null_items\"),\n",
    "    ]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el esquema para verificar que la columna `items` es un array o lista\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88c9692-f8e8-45b0-98ac-58e9a12b14e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Se necesita transformar la cadena en una lista para que FP-Growth pueda procesarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Convertir la columna 'items' de STRING a ARRAY<STRING>\n",
    "df_transactions = df_transactions.withColumn(\"items\", split(col(\"items\"), \",\"))\n",
    "\n",
    "# Verificar el esquema y los datos\n",
    "df_transactions.printSchema()\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# Crear el modelo FP-Growth y entrenarlo\n",
    "fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.05, minConfidence=0.7)\n",
    "\n",
    "model = fp_growth.fit(df_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos frecuentes\n",
    "frequent_itemsets = model.freqItemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de asociación\n",
    "association_rules = model.associationRules.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar número de reglas de asociación\n",
    "association_rules.take(1)  \n",
    "print(association_rules.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DF con reglas de asociacion\n",
    "rules_path = f\"{base_path}/resultados/fpgrowth_rules.parquet\"\n",
    "\n",
    "association_rules.write.mode(\"overwrite\").parquet(rules_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el dataset guardado\n",
    "saved_rules = spark.read.parquet(rules_path)\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "saved_rules.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
