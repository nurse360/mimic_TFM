{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTzxvw2zvyTZ"
   },
   "source": [
    "# Preparación datos para introducir al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlPjumFVrtE6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, regexp_extract, count\n",
    "\n",
    "# Detener sesión anterior\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Preparación dataset para entrenamiento del modelo predictivo\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxCGQnwYr-qy"
   },
   "outputs": [],
   "source": [
    "ruta_dataset = \"data/resultados/dataset_final.parquet\"\n",
    "ruta_rules = \"data/resultados/fpgrowth_rules.parquet\"\n",
    "ruta_arbol = \"data/resultados/dataset_arbol.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pokmwnTPsYuu",
    "outputId": "150559e0-4a62-443a-aed9-e4c27d5eae82"
   },
   "outputs": [],
   "source": [
    "# Leer dataset final (original con todos los datos de ingresos)\n",
    "df_dataset = spark.read.parquet(ruta_dataset)\n",
    "print(\"Dataset original cargado correctamente.\")\n",
    "df_dataset.printSchema()\n",
    "df_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.filter(col(\"edad\") == 91).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvtZYUrUssyE",
    "outputId": "67a96da6-e2b3-421e-915e-1de24ed14407"
   },
   "outputs": [],
   "source": [
    "# Leer dataset reglas\n",
    "df_rules = spark.read.parquet(ruta_rules)\n",
    "print(\"Dataset reglas cargado correctamente.\")\n",
    "df_rules.printSchema()\n",
    "df_rules.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqFU1AjINtz3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d1f405ba-8390-4838-aeae-2cd1e0f53897"
   },
   "outputs": [],
   "source": [
    "# Leer dataset arbol\n",
    "df_arbol = spark.read.parquet(ruta_arbol)\n",
    "print(\"Dataset para el modelo predictivo cargado correctamente.\")\n",
    "df_arbol.printSchema()\n",
    "df_arbol.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJE-7w9Ps0Zd",
    "outputId": "173840ab-89fb-4f6a-f117-9036556c01f2"
   },
   "outputs": [],
   "source": [
    "# Verificar registros de cada dataset\n",
    "print(f\" Registros en DATASET FINAL: {df_dataset.count()}\")\n",
    "print(f\" Registros en DATASET REGLAS GENERADAS: {df_rules.count()}\")\n",
    "print(f\" Registros en DATASET ARBOL: {df_arbol.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_arbol.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdpMK2aNRyec"
   },
   "source": [
    "# Análisis exploratorio de los datos del arbol de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns5N4yJQweje",
    "outputId": "420fc9b6-6620-4979-8304-aa313d72ce94"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Extraer los diagnósticos únicos desde la columna \"dominios\"\n",
    "df_diagnosticos = df_arbol.select(explode(col(\"dominios\")).alias(\"diagnostico\")).distinct()\n",
    "\n",
    "# Contar la cantidad de diagnósticos únicos\n",
    "num_diagnosticos = df_diagnosticos.count()\n",
    "print(f\"Total de diagnósticos únicos: {num_diagnosticos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXE4xv-pScgG",
    "outputId": "2a6f2ff0-a41a-4164-9866-2cda87b526a3"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# Contar cuántos diagnósticos tiene cada paciente\n",
    "df_pacientes_diagnosticos = df_arbol.withColumn(\"num_diagnosticos\", size(col(\"dominios\")))\n",
    "\n",
    "# Mostrar estadísticas generales\n",
    "df_pacientes_diagnosticos.select(\"num_diagnosticos\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPVW9EXKVQ_b"
   },
   "outputs": [],
   "source": [
    "# Crear una nueva columna con el número de diagnósticos por paciente\n",
    "df_arbol = df_arbol.withColumn(\"num_diagnosticos\", size(\"dominios\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "oCB7dwkeUez3",
    "outputId": "d80e90eb-4bcf-4e43-e93e-ea7a9c8da037"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extraer los datos de la columna \"num_diagnosticos\"\n",
    "num_diagnosticos = df_arbol.select(\"num_diagnosticos\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(num_diagnosticos, bins=30, kde=True, color='#99CC99', alpha=0.7, edgecolor='white')\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "plt.xlabel(\"Número de Diagnósticos por Paciente\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribución del Número de Diagnósticos por Paciente\")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veGvlcwhV_AK"
   },
   "source": [
    "El histograma muestra la distribución del número de diagnósticos por paciente, evidenciando que la mayoría de los pacientes tienen entre 5 y 15 diagnósticos. Se observa una ligera asimetría a la derecha, lo que indica que hay pacientes con una cantidad de diagnósticos significativamente mayor al promedio. Además, la curva KDE refuerza la presencia de ciertos valores más frecuentes, con un pico alrededor de 10 diagnósticos.\n",
    "\n",
    "Este análisis nos sugiere que podría ser útil agrupar a los pacientes según su número de diagnósticos para definir una métrica de complejidad diagnóstica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpELe2HrVBUw",
    "outputId": "de17fd98-b01e-470d-8ca1-8ec7faee6fce"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "# Obtener los cuantiles del número de diagnósticos\n",
    "quantiles = df_arbol.approxQuantile(\"num_diagnosticos\", [0.25, 0.5, 0.75, 0.9], 0.01)\n",
    "\n",
    "# Definir los umbrales\n",
    "q25, q50, q75, q90 = quantiles\n",
    "\n",
    "# Crear la columna de complejidad diagnóstica basada en los cuantiles\n",
    "df_arbol = df_arbol.withColumn(\n",
    "    \"complejidad_diagnostica\",\n",
    "    when(col(\"num_diagnosticos\") <= q25, \"Baja\")\n",
    "    .when((col(\"num_diagnosticos\") > q25) & (col(\"num_diagnosticos\") <= q75), \"Media\")\n",
    "    .when((col(\"num_diagnosticos\") > q75) & (col(\"num_diagnosticos\") <= q90), \"Alta\")\n",
    "    .otherwise(\"Muy Alta\")\n",
    ")\n",
    "\n",
    "df_arbol.groupBy(\"complejidad_diagnostica\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "_CJkp-0wWWX7",
    "outputId": "fd8aa8df-7095-4ced-bb07-1298e67e6cbb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convertir los datos a Pandas para graficar\n",
    "df_complejidad_pandas = df_arbol.groupBy(\"complejidad_diagnostica\").count().toPandas()\n",
    "\n",
    "# Ordenar categorías en el gráfico\n",
    "orden_categorias = [\"Baja\", \"Media\", \"Alta\", \"Muy Alta\"]\n",
    "df_complejidad_pandas = df_complejidad_pandas.set_index(\"complejidad_diagnostica\").loc[orden_categorias].reset_index()\n",
    "\n",
    "# Configurar el estilo de la visualización\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    data=df_complejidad_pandas,\n",
    "    x=\"complejidad_diagnostica\",\n",
    "    y=\"count\",\n",
    "    alpha=0.7,\n",
    "    edgecolor='black',\n",
    "    color=\"#99CC99\"\n",
    ")\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(\"Nivel de Complejidad Diagnóstica\", fontsize=12)\n",
    "plt.ylabel(\"Número de Pacientes\", fontsize=12)\n",
    "plt.title(\"Distribución de Pacientes por Complejidad Diagnóstica\", fontsize=14)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "hHryrkKuWjgc",
    "outputId": "fde49d2a-f80e-4943-d169-bf264e3150f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de variables categóricas a visualizar\n",
    "variables_categoricas = [\"edad_categoria\", \"sexo\", \"estado_civil\", \"tipo_seguro\", \"grupo_poblacional\", \"muerte_durante_ingreso\"]\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas (solo con las columnas necesarias)\n",
    "df_pandas = df_arbol.select(variables_categoricas).toPandas()\n",
    "\n",
    "# Configuración de los gráficos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generar gráficos de barras para cada variable categórica\n",
    "for i, var in enumerate(variables_categoricas):\n",
    "    df_pandas[var].value_counts().plot(kind=\"bar\", ax=axes[i], alpha=0.7, edgecolor=\"white\")\n",
    "    axes[i].set_title(f\"Distribución de {var}\")\n",
    "    axes[i].set_ylabel(\"Frecuencia\")\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0duCiSyZiFN"
   },
   "source": [
    "## Modelo XGBoost para un problema *multilabel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_MUxMOhZnaa"
   },
   "source": [
    "En este dataset, cada observación puede estar asociada simultáneamente a múltiples etiquetas, lo que es especialmente relevante en el contexto clínico analizado, donde un mismo paciente puede presentar diagnósticos pertenecientes a distintos dominios ICD-10. Para abordar este problema, es necesario aplicar técnicas específicas de clasificación multilabel que permitan al modelo aprender las relaciones entre las características (parámetros bioquímicos y otras variables) y múltiples etiquetas (dominios o capítulos de diagnóstico). Este enfoque es clave para no perder información relevante en el proceso de predicción.\n",
    "\n",
    "La ha elegido XGBoost como algoritmo principal por varias razones. En primer lugar, XGBoost admite valores faltantes de manera nativa, lo que elimina la necesidad de imputar los valores NaN en los datos de entrada. En segundo lugar, su capacidad para manejar directamente problemas multilabel permite entrenar un modelo eficiente que tenga en cuenta la posible relación entre las etiquetas. Además, su enfoque basado en árboles de decisión es robusto frente a datos tabulares como el que se analiza, con características heterogéneas y correlacionadas.\n",
    "\n",
    "El modelo generado será capaz de predecir múltiples capítulos de diagnóstico para un paciente dado.\n",
    "\n",
    "Este enfoque permitirá una clasificación más precisa y granular, facilitando aplicaciones prácticas como la predicción de patrones de diagnóstico, la identificación de combinaciones atípicas de capítulos, y la mejora en la toma de decisiones clínicas. El procedimiento descrito no solo es una solución técnicamente adecuada para este problema, sino que también alinea las características del dataset con un enfoque robusto y escalable. La aplicación de un modelo multilabel basado en XGBoost permitirá obtener resultados precisos y relevantes, maximizando el aprovechamiento de la información disponible en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqcbYJGjZoNU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1c653e1d-92bd-46d6-b64f-9eedd6700da5"
   },
   "outputs": [],
   "source": [
    "df_arbol.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estados_civiles = df_arbol.select(\"estado_civil\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos_seguro = df_arbol.select(\"tipo_seguro\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupos_poblacionales = df_arbol.select(\"grupo_poblacional\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edad_categorias = df_arbol.select(\"edad_categoria\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSH8HFYHc3Zn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c9451f34-66ef-4278-e8b9-d6510f6a540c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, array, struct, lit, expr\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Crear una copia del DataFrame original antes de hacer modificaciones\n",
    "df_procesado = df_arbol\n",
    "\n",
    "# Eliminar las variables que no se van a introducir en el modelo\n",
    "columnas_a_eliminar = [\"id_ingreso\"]\n",
    "df_procesado = df_procesado.drop(*columnas_a_eliminar)\n",
    "\n",
    "# Convertir \"sexo\" a numérico (0 = M, 1 = F)\n",
    "df_procesado = df_procesado.withColumn(\"sexo\", when(col(\"sexo\") == \"F\", 1).otherwise(0))\n",
    "\n",
    "# Convertir \"muerte_durante_ingreso\" a tipo numérico\n",
    "df_procesado = df_procesado.withColumn(\"muerte_durante_ingreso\", col(\"muerte_durante_ingreso\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# Definir las columnas categóricas y sus valores únicos\n",
    "columnas_categoricas = {\n",
    "    \"estado_civil\": estados_civiles,\n",
    "    \"tipo_seguro\": tipos_seguro,\n",
    "    \"edad_categoria\": edad_categorias,\n",
    "    \"grupo_poblacional\": grupos_poblacionales\n",
    "}\n",
    "\n",
    "# Generar expresiones para las nuevas columnas\n",
    "df_procesado = df_procesado.withColumns(\n",
    "    {f\"{col}_{val}\": expr(f\"IF({col} = '{val}', 1, 0)\") for col, vals in columnas_categoricas.items() for val in vals}\n",
    ")\n",
    "\n",
    "df_procesado = df_procesado.drop('estado_civil', 'tipo_seguro', 'grupo_poblacional', 'complejidad_diagnostica', 'edad_categoria')\n",
    "    \n",
    "# Verificar el esquema del DataFrame después de la transformación\n",
    "df_procesado.printSchema()\n",
    "df_procesado.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRB1xi-Gicds"
   },
   "source": [
    "Para el preprocesamiento de los datos para su uso en modelos de aprendizaje automático, se han eliminado variables irrelevantes, convertido variables categóricas en un formato numérico mediante One-Hot Encoding, y normalizado las variables binarias. Se ha utilizado un Pipeline para aplicar las transformaciones de manera eficiente en PySpark, asegurando que el dataset esté correctamente estructurado antes de entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmJCefMlsbTE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "83a49cef-7cc6-442b-c484-4a99bfda33d0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Obtener la frecuencia de cada diagnóstico\n",
    "df_freq = df_procesado.select(explode(col(\"dominios\")).alias(\"diagnostico\"))\n",
    "df_freq = df_freq.groupBy(\"diagnostico\").count()\n",
    "\n",
    "# Filtrar los diagnósticos que aparecen al menos en 200 pacientes\n",
    "df_freq = df_freq.filter(col(\"count\") >= 200)\n",
    "\n",
    "# Obtener la lista de diagnósticos más frecuentes\n",
    "diagnosticos_frecuentes = [row[\"diagnostico\"] for row in df_freq.collect()]\n",
    "print(f\"Número de diagnósticos después del filtrado: {len(diagnosticos_frecuentes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_procesado.select(\"dominios\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnoA1VxakROS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d7a377c7-5260-4b49-8131-1526d4232075"
   },
   "outputs": [],
   "source": [
    "# Binarizar los diagnosticos\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Conservar las columnas existentes sin \"dominios\"\n",
    "columnas_existentes = [col(c) for c in df_procesado.columns if c != \"dominios\"]\n",
    "\n",
    "# Generar las nuevas columnas binarias\n",
    "columnas_diagnosticos = [expr(f\"IF(array_contains(dominios, '{diag}'), 1, 0) AS `{diag}`\") for diag in diagnosticos_frecuentes]\n",
    "\n",
    "# Aplicar la transformación de manera eficiente en una sola operación\n",
    "df_procesado = df_procesado.select(*columnas_existentes, *columnas_diagnosticos)\n",
    "\n",
    "# Eliminar la columna original \"dominios\"\n",
    "df_procesado = df_procesado.drop(\"dominios\")\n",
    "\n",
    "# Verificar la estructura final\n",
    "df_procesado.printSchema()\n",
    "df_procesado.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVz6j4-XvcRF",
    "outputId": "da7f060e-de59-4aa5-82e9-534b8a8d62f5"
   },
   "outputs": [],
   "source": [
    "# Obtener todas las columnas del DataFrame\n",
    "columnas = df_procesado.columns\n",
    "\n",
    "# Verificar si hay nombres duplicados\n",
    "from collections import Counter\n",
    "contador_columnas = Counter(columnas)\n",
    "columnas_duplicadas = [col for col, count in contador_columnas.items() if count > 1]\n",
    "\n",
    "print(f\"Columnas duplicadas: {columnas_duplicadas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6ltSxF6kRtI",
    "outputId": "401950c0-0f62-498c-9a24-59f24cab6187"
   },
   "outputs": [],
   "source": [
    "ruta_guardado = \"data/resultados/arbol_preprocesado.parquet\"\n",
    "df_procesado.write.mode(\"overwrite\").parquet(ruta_guardado)\n",
    "\n",
    "print(\"Dataset guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
